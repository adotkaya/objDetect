{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "345e5bd0-c183-4b2d-9495-fe35056b7f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\kayal\\anaconda3\\lib\\site-packages (4.8.0.74)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\kayal\\anaconda3\\lib\\site-packages (from opencv-python) (1.20.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fb86162-edd5-4d06-97a5-8d0368536c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\kayal\\anaconda3\\lib\\site-packages (4.8.0.74)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\kayal\\anaconda3\\lib\\site-packages (from opencv-python) (1.20.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "874bcd0f-d2a6-4926-a8fc-c9c0678815ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIDENCE = 0.5\n",
    "SCORE_THRESHOLD = 0.5\n",
    "IOU_THRESHOLD = 0.5\n",
    "config_path = \"obj_detect/yolov3.cfg\"\n",
    "weights = \"obj_detect/yolov3.weights\"\n",
    "labels = open(\"obj_detect/coco.names\").read().strip().split(\"\\n\")\n",
    "colors = np.random.randint(0, 255, size=(len(labels), 3), dtype=\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "61e6b420-12d5-4b1b-ac37-6c4c5b2c2ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cv2.dnn.readNetFromDarknet(config_path, weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e8afd754-b167-4704-9b58-0e173241ec10",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-f7c77c611843>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[0mboxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfidences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;31m# Draw bounding boxes and labels on the frame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-70-f7c77c611843>\u001b[0m in \u001b[0;36mmodel_output\u001b[1;34m(frame)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# Use the list of output layer names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0moutput_blobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_layer_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mboxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfidences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the pre-trained YOLO model and its configuration\n",
    "net = cv2.dnn.readNet(\"obj_detect/yolov3.cfg\", \"obj_detect/yolov3.weights\")\n",
    "\n",
    "# Assuming you have a list of output layer names\n",
    "output_layer_names = net.getUnconnectedOutLayersNames()\n",
    "\n",
    "def model_output(frame):\n",
    "    h, w = frame.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1/255.0, (416, 416), swapRB=True, crop=False)\n",
    "\n",
    "    net.setInput(blob)\n",
    "\n",
    "    # Use the list of output layer names\n",
    "    output_blobs = net.forward(output_layer_names)\n",
    "\n",
    "    boxes, confidences, class_ids = [], [], []\n",
    "\n",
    "    for output in output_blobs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > CONFIDENCE:\n",
    "                box = detection[0:4] * np.array([w, h, w, h])\n",
    "                (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "                x = int(centerX - (width / 2))\n",
    "                y = int(centerY - (height / 2))\n",
    "                boxes.append([x, y, int(width), int(height)])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    return boxes, confidences, class_ids\n",
    "\n",
    "CONFIDENCE = 0.5\n",
    "\n",
    "# Open the video file\n",
    "video_path = \"obj_detect/video.mp4\"\n",
    "video_capture = cv2.VideoCapture(video_path)\n",
    "\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    boxes, confidences, class_ids = model_output(frame)\n",
    "\n",
    "    # Draw bounding boxes and labels on the frame\n",
    "    for i in range(len(boxes)):\n",
    "        x, y, w, h = boxes[i]\n",
    "        label = f\"Class {class_ids[i]}, Confidence: {confidences[i]:.2f}\"\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Object Detection\", frame)\n",
    "\n",
    "    # Break the loop if 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release video capture and close all OpenCV windows\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569ee8e9-cf68-4a45-9417-3464f5fca471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_recognition(path_name):\n",
    "    image = cv2.imread(path_name)\n",
    "    boxes, confidences, class_ids = model_output(path_name)\n",
    "    idxs = cv2.dnn.NMSBoxes(boxes, confidences, SCORE_THRESHOLD, IOU_THRESHOLD)\n",
    "    font_scale = 1\n",
    "    thickness = 1\n",
    "    if len(idxs)>0:\n",
    "        for i in idxs.flatten():\n",
    "            x,y = boxes[i][0], boxes[i][1]\n",
    "            w,h = boxes[i][2], boxes[i][3]\n",
    "            color = [int(c) for c in colors[class_ids[i]]]\n",
    "            cv2.rectangle(image, (x,y), (x+w, y+h), color = color, thickness= thickness)\n",
    "            text = f\"{labels[class_ids[i]]}: {confidences[i]:.2f}\"\n",
    "            (text_width, text_height) = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, fontScale=font_scale, thickness=thickness)[0]\n",
    "            text_offset_x = x\n",
    "            text_offset_y = y - 5\n",
    "            box_coords = ((text_offset_x, text_offset_y), (text_offset_x + text_width + 2, text_offset_y - text_height))\n",
    "            overlay = image.copy()\n",
    "            cv2.rectangle(overlay, box_coords[0], box_coords[1], color=color, thickness=cv2.FILLED)\n",
    "            image = cv2.addWeighted(overlay, 0.6, image, 0.4, 0)#add two copies to one and the numbers define blending percent\n",
    "            cv2.putText(image, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            fontScale=font_scale, color=(0, 0, 0), thickness=thickness)\n",
    "    %matplotlib inline\n",
    "    plt.rcParams['figure.figsize'] = (10.0, 10.0)\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "201fca2b-746c-4a8f-be9c-a3639df2b633",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'detection_recognition' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-256568fb1ce4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdetection_recognition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"obj_detect/video.mp4\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'detection_recognition' is not defined"
     ]
    }
   ],
   "source": [
    "detection_recognition(\"obj_detect/video.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
